from crewai import Agent, Task, Crew, Process
from langchain_ollama import OllamaLLM
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.tools import Tool
from crewai_tools import FileReadTool, FileWriteTool, CodeInterpreterTool
from langchain_community.tools import SerperDevTool
from sentence_transformers import SentenceTransformer, util
import numpy as np
import os
import json
from fastapi import FastAPI
from pydantic import BaseModel
from peft import LoraConfig, get_peft_model
from unsloth import FastLanguageModel
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from datasets import Dataset
import torch

os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")

# Local RAG Cache Setup
def init_local_cache(cache_dir=os.path.expanduser("~/.ai_cache")):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=cache_dir, embedding_function=embeddings)
    return vectorstore

cache = init_local_cache()

def local_search(query, k=3):
    results = cache.similarity_search(query, k=k)
    if results:
        return "\n".join([doc.page_content for doc in results])
    else:
        search_tool = SerperDevTool()
        web_results = json.loads(search_tool.run(query))  # Parse JSON
        snippets = "\n".join([item.get('snippet', '') for item in web_results.get('organic', []) if 'snippet' in item])
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = [Document(page_content=snippets)]
        splits = splitter.split_documents(docs)
        cache.add_documents(splits)
        cache.persist()
        return snippets

local_search_tool = Tool(name="LocalSearch", description="Search cached knowledge or web (caches auto).", func=local_search)

# LLMs
planner_llm = OllamaLLM(model="phi3:medium")
backend_llm = OllamaLLM(model="mistral-nemo:12b")
uiux_llm = OllamaLLM(model="gemma2:9b")
frontend_llm = OllamaLLM(model="llama3.1:8b")
tester_llm = OllamaLLM(model="phi3:medium")

# Tools
read_tool = FileReadTool()
write_tool = FileWriteTool()
code_tool = CodeInterpreterTool()

# Agents (Add local_search_tool to each)
planner = Agent(
    role="Product Manager", goal="Plan & delegate, incorporating LocalSearch research", backstory="Agile PM who scouts trends.",
    llm=planner_llm, tools=[local_search_tool, read_tool, write_tool], verbose=True
)
backend_eng = Agent(
    role="Backend Engineer", goal="Code APIs/DB, research libs via LocalSearch", backstory="Python pro who debugs live.",
    llm=backend_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)
uiux_designer = Agent(
    role="UI/UX Designer", goal="Design wireframes with LocalSearch inspo", backstory="Figma pro for accessibility.",
    llm=uiux_llm, tools=[local_search_tool, read_tool, write_tool], verbose=True
)
frontend_eng = Agent(
    role="Frontend Engineer", goal="Implement UIs, integrate via LocalSearch", backstory="React expert.",
    llm=frontend_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)
tester = Agent(
    role="QA Engineer", goal="Test code, score for tuning", backstory="TDD meticulous.",
    llm=tester_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)
devops = Agent(
    role="DevOps Engineer", goal="Deploy/maintain with LocalSearch patches", backstory="Infra automator.",
    llm=planner_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)

# Lang Specialists
swift_agent = Agent(
    role="iOS Engineer", goal="SwiftUI code, convert paradigms via LocalSearch", backstory="Swift mapper.",
    llm=frontend_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)
kotlin_agent = Agent(
    role="Android Engineer", goal="Kotlin/Jetpack, convert from Swift/C++", backstory="MVVM expert.",
    llm=backend_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)
cpp_agent = Agent(
    role="Desktop Engineer", goal="C++/Qt, webify bridges", backstory="Cross-desktop pro.",
    llm=uiux_llm, tools=[local_search_tool, code_tool, read_tool, write_tool], verbose=True
)

# Tasks
plan_task = Task(
    description="Research & spec '{app_idea}' via LocalSearch (e.g., similar apps, tech 2025). Output Markdown roadmap.",
    expected_output="spec.md with tasks.", agent=planner
)
backend_proposal_task = Task(
    description="From spec, draft backend. Use LocalSearch for best practices. Output JSON proposal.",
    expected_output="backend_proposal.json", agent=backend_eng, context=[plan_task]
)
uiux_task = Task(
    description="From spec, design wireframes. Use LocalSearch for inspo. Output Mermaid diagrams.",
    expected_output="designs folder.", agent=uiux_designer, context=[plan_task]
)
frontend_task = Task(
    description="From designs/backend, build frontend. Use LocalSearch for integrations.",
    expected_output="frontend code.", agent=frontend_eng, context=[uiux_task, backend_proposal_task]
)
council_task = Task(
    description="Council: Review proposals. Debate/vote via weighted similarity (higher weight to specialists). Use council_debate helper. Output merged code.",
    expected_output="council_merge.json", agent=tester, context=[backend_proposal_task, frontend_task], async_execution=True
)

def council_debate(proposals, weights={'backend': 1.5, 'frontend': 1.2}):  # Weighted for specialists
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeds = {k: model.encode(v) for k, v in proposals.items()}
    mean_embed = np.mean([w * embeds[k] for k, w in weights.items() if k in embeds], axis=0)
    scores = {k: util.cos_sim(embeds[k], mean_embed) for k in proposals}
    winner = max(scores, key=scores.get)
    return {"verdict": proposals[winner], "scores": scores}

convert_task = Task(
    description="From {source_platform} code in {code}, generate {target_platform} variant. Use LocalSearch for mappings. Council-vote merges.",
    expected_output="Converted code + diff.", agent=swift_agent, context=[council_task], async_execution=True
)
improve_task = Task(
    description="Review outputs: Score quality. Generate dataset for LoRA if score < 0.8. Index to cache.",
    expected_output="Adapter + report", agent=tester, context=[convert_task]
)
maintain_task = Task(
    description="From final app, generate monitor/deploy scripts. Use LocalSearch for patches.",
    expected_output="maintain.py + deploy.sh", agent=devops, context=[council_task]
)

# Crew
dev_crew = Crew(
    agents=[planner, backend_eng, uiux_designer, frontend_eng, tester, devops, swift_agent, kotlin_agent, cpp_agent],
    tasks=[plan_task, backend_proposal_task, uiux_task, frontend_task, council_task, convert_task, improve_task, maintain_task],
    process=Process.sequential, verbose=2, memory=True
)

# PEFT Self-Improvement
def fine_tune_if_needed(result):
    if 'fine_tune' in result and result['score'] < 0.8:
        try:
            model, tokenizer = FastLanguageModel.from_pretrained("meta-llama/Llama-3.1-8B", dtype=torch.float16, load_in_4bit=True)
            peft_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"], lora_dropout=0.05)
            model = get_peft_model(model, peft_config)
            train_ds = Dataset.from_list(result['dataset'])
            batch_size = 2 if torch.cuda.mem_get_info()[0] > 8e9 else 1
            trainer = SFTTrainer(model=model, train_dataset=train_ds, tokenizer=tokenizer, args=TrainingArguments(output_dir="lora-adapter", num_train_epochs=1, per_device_train_batch_size=batch_size))
            trainer.train()
            model.save_pretrained(os.path.expanduser("~/.ollama/lora-crew"))
        except torch.cuda.OutOfMemoryError:
            pass  # Skip tune

# FastAPI
app = FastAPI()

class Idea(BaseModel):
    idea: str

class Convert(BaseModel):
    source: str
    target: str
    code_path: str

@app.post("/build-app")
def kickoff_crew(idea: Idea):
    result = dev_crew.kickoff(inputs={'app_idea': idea.idea})
    fine_tune_if_needed(result)
    return {"output_path": "./output", "summary": str(result)}

@app.post("/convert")
def convert_code(convert: Convert):
    with open(convert.code_path, 'r') as f:
        code = f.read()
    result = dev_crew.kickoff(inputs={'source_platform': convert.source, 'target_platform': convert.target, 'code': code})
    fine_tune_if_needed(result)
    return {"converted_code": result['output'], "diff": "Key changes"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
